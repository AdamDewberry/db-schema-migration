import pandas as pd
import yaml

colunn_subset = [
    "TABLE_CATALOG",
    "TABLE_SCHEMA",
    "TABLE_NAME",
    "COLUMN_NAME",
    "DATA_TYPE",
    "IS_NULLABLE",
    "CONSTRAINT_TYPE",
    "COLUMN_DEFAULT",
]
source_schema = (
    pd.read_csv("schema.csv")
    # Remove white space from column names
    .rename(columns=lambda x: x.strip())
    # Remove white space from rows
    .map(lambda x: x.strip() if isinstance(x, str) else x)
    # For all rows
    .replace({"NULL": False})[colunn_subset]
)
# Data cleaning to conform to boolean types
source_schema.IS_NULLABLE = source_schema.IS_NULLABLE.apply(
    lambda x: True if x == "YES" else False
)


def get_constraints(column_attributes):
    constraints = {"nullable": column_attributes["IS_NULLABLE"]}
    if column_attributes["CONSTRAINT_TYPE"] == "UNIQUE":
        constraints.update({"unique": True})
    # Experimental,
    if (
        column_attributes["COLUMN_DEFAULT"]
        and column_attributes["DATA_TYPE"] == "varchar"
    ):
        constraints.update({"defaultValue": column_attributes["COLUMN_DEFAULT"][2:-2]})
    if column_attributes["COLUMN_DEFAULT"] and column_attributes["DATA_TYPE"] == "int":
        constraints.update(
            {"defaultValue": int(column_attributes["COLUMN_DEFAULT"][2:-2])}
        )
    if column_attributes["CONSTRAINT_TYPE"] == "PRIMARY KEY":
        constraints = {"primaryKey": True}
    return constraints


# Add any additional Attributes or Preconditions as dictionaries in the list below
change_log = {
    "databaseChangeLog": [
        {"objectQuotingStrategy": "QUOTE_ALL_OBJECTS"}
        # , {attributes}
        # , {precondtions}
    ]
}

catalogue = (
    source_schema[["TABLE_CATALOG", "TABLE_SCHEMA", "TABLE_NAME"]]
    .drop_duplicates()
    .to_dict("records")
)

for table in catalogue:
    database_name = table["TABLE_CATALOG"]
    schema_name = table["TABLE_SCHEMA"]
    table_name = table["TABLE_NAME"]

    change_log["databaseChangeLog"].append(
        {
            "changeSet": {
                "id": f"{database_name}-{schema_name}-{table_name}-v0.1.0",
                "author": "Generated by Adam",
                "changes": {
                    "createTable": {
                        "catalogName": database_name,
                        "schemaName": schema_name,
                        "tableName": table_name,
                        "columns": [
                            {
                                "column": {
                                    "name": column_attributes["COLUMN_NAME"],
                                    "type": column_attributes["DATA_TYPE"],
                                    "constraints": get_constraints(column_attributes),
                                }
                            }
                            for column_attributes in source_schema[
                                source_schema["TABLE_NAME"] == table_name
                            ].to_dict("records")
                        ]
                        ## To include extra default columns in each table, uncomment the lines below
                        # + [
                        #     {
                        #         "column": {
                        #             "name": "source_systen",
                        #             "type": "VARCHAR",
                        #             "constraints": {"nullable": False},
                        #         }
                        #     }
                        # ],
                    }
                },
            }
        }
    )

with open("changelog.yaml", "w") as yaml_file:
    yaml.dump(
        change_log,
        yaml_file,
        allow_unicode=True,
        default_flow_style=False,
        sort_keys=False,
    )
